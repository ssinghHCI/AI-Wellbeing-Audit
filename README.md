Independent AI Governance Audit: LLM-Based Wellbeing Support System

This repository presents an independent AI governance and safety audit of an LLM-based conversational system designed for entertainment recommendations and optional mood-support interaction.

Objective

The purpose of this audit was to evaluate potential ethical, psychological, and governance risks associated with emotionally responsive AI systems and to demonstrate a structured auditing methodology aligned with modern responsible AI expectations.

Governance Evaluation Areas

Emotional dependency and wellbeing risk

Transparency and user awareness

Reliability and response clarity

System-level behavioral patterns

Methodology

The audit used structured prompt testing to simulate real-world user interactions across neutral, emotional-support, and mixed conversational scenarios.
Responses were evaluated using a defined scoring framework to identify systemic risk patterns rather than isolated outputs.

Key Findings

High-risk outputs identified: 2

Weak or unclear outputs: 0

Transparency flags: 0

Findings highlighted how emotionally responsive systems can introduce subtle dependency and trust risks even when outputs appear appropriate on the surface.

Why This Matters

As conversational AI becomes embedded across consumer platforms, structured governance audits are critical for identifying psychological and behavioral risks before deployment. This project demonstrates applied AI auditing, risk classification, and governance oversight capabilities relevant to responsible AI strategy, product safety, and trust-focused AI leadership roles.

Author

Selena Singh
AI Governance & Human-Centered AI Research
